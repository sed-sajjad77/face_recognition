{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c42322",
   "metadata": {},
   "source": [
    "# 0. Getting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a445e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff0d84",
   "metadata": {},
   "source": [
    "Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9c321",
   "metadata": {},
   "source": [
    "# 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04964bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Setup path to data folder\n",
    "image_path = Path(\"data/\")\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(\"Please create data from add_person.py and addRandomFace.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32828c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dirs\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "valid_dir = image_path / \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0004c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    np.float32,\n",
    "    # Flip the images randomly on the horizontal\n",
    "    # transforms.RandomHorizontalFlip(p=0.2), # p = probability of flip, 0.2 = 20% chance\n",
    "    # Randomly select a rectangle region in the input image and erase its pixels.\n",
    "    # transforms.RandomErasing(p=0.2),\n",
    "    # Randomly change the brightness, contrast, saturation and hue of an imag\n",
    "    # transforms.ColorJitter(),\n",
    "    # Turn the image into a torch.Tensor\n",
    "     # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "    fixed_image_standardization,\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import data_setup\n",
    "# Create training and testing DataLoaders as well as get a list of class names\n",
    "train_dataloader, valid_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 valid_dir=valid_dir,                  \n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
    "                                                                                                 batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bda9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468eee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths. \n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
    "plot_transformed_images(image_path_list, \n",
    "                        transform=manual_transforms, \n",
    "                        n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35a113",
   "metadata": {},
   "source": [
    "### Define Inception Resnet V1 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(InceptionResnetV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a57464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained resnet model\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes=len(class_names)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=resnet, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd3a25",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a091d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, [5, 10])\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, valid_dataloader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_dataloader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, valid_dataloader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95374dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021da6d",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd33db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import save_model\n",
    "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
    "                 exist_ok=True # if models directory already exists, don't error\n",
    ")\n",
    "# Create model save path\n",
    "MODEL_NAME = \"vggface.pth\"\n",
    "\n",
    "save_model(model=resnet,\n",
    "            target_dir=\"models\",\n",
    "            model_name=MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
